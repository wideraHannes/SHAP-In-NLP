{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/Users/johanneswidera/Uni/bachelorarbeit/Code/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import contractions\n",
    "import re\n",
    "import string\n",
    "from helper import read_imdb_split,download_data\n",
    "from model_helper.tf_idf import build_tf_idf\n",
    "from model_helper.PipelineWrapper import PipelineWrapper\n",
    "from custom_shap_explainer.custom_global import custom_global_explanation\n",
    "from custom_shap_explainer.signal_words import highlight_signal_words\n",
    "np.random.seed(1337)\n",
    "shap.initjs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data()\n",
    "corpus_train, y_train = read_imdb_split('../data/aclImdb/train')\n",
    "corpus_test, y_test = read_imdb_split('../data/aclImdb/test')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tf_idf = build_tf_idf()\n",
    "vectorizer_tf_idf.fit(corpus_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Build Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(penalty=\"l2\")\n",
    "model_logregression_new = PipelineWrapper(model, vectorizer_tf_idf, corpus_test, corpus_train, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logregression_new.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logregression_new.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_logreg = model_logregression_new.predict_proba(corpus_test)\n",
    "\n",
    "\n",
    "LOGREG_PREDICTION_FILE = 'predictions_logreg.csv'\n",
    "\n",
    "file_exists = os.path.exists(LOGREG_PREDICTION_FILE)\n",
    "\n",
    "if not file_exists:\n",
    "    # Calculate BERT predictions\n",
    "    \n",
    "    predictions_logreg = model_logregression_new.predict_proba(corpus_test)\n",
    "\n",
    "# Destructure probabilities for class_1 and class_2\n",
    "    predictions_class_0 = predictions_logreg[:, 0]\n",
    "    predictions_class_1 = predictions_logreg[:, 1]\n",
    "    \n",
    "    # Create a DataFrame containing the test samples and BERT predictions\n",
    "    results = pd.DataFrame({\n",
    "    'logreg_0': predictions_class_0,\n",
    "    'logreg_1': predictions_class_1\n",
    "    })\n",
    "    # Create a DataFrame containing the test samples and BERT predictions\n",
    "  \n",
    "    # Save the DataFrame to a CSV file\n",
    "    results.to_csv(LOGREG_PREDICTION_FILE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Build Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "model_dtc = PipelineWrapper(DecisionTreeClassifier(min_samples_leaf=20), vectorizer_tf_idf, corpus_test, corpus_train, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dtc.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dtc.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_DTC = model_dtc.predict_proba(corpus_test[:1])\n",
    "predictions_DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC_PREDICTION_FILE = 'predictions_dtc.csv'\n",
    "\n",
    "file_exists = os.path.exists(DTC_PREDICTION_FILE)\n",
    "\n",
    "if not file_exists:\n",
    "    # Calculate DTC predictions\n",
    "    predictions_DTC = model_dtc.predict_proba(corpus_test)\n",
    "\n",
    "# Destructure probabilities for class_1 and class_2\n",
    "    predictions_class_0 = predictions_DTC[:, 0]\n",
    "    predictions_class_1 = predictions_DTC[:, 1]\n",
    "    \n",
    "    # Create a DataFrame containing the test samples and BERT predictions\n",
    "    results = pd.DataFrame({\n",
    "    'dtc_0': predictions_class_0,\n",
    "    'dtc_1': predictions_class_1\n",
    "    })\n",
    "    results.to_csv(DTC_PREDICTION_FILE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Build Fine Tuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../../FineTunedBERT/Sentiment/24060142\")\n",
    "# Load model\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"../../FineTunedBERT/Sentiment/24060142\")\n",
    "model_bert = pipeline('sentiment-analysis', model=loaded_model, tokenizer=tokenizer, max_length=512, truncation=True, top_k=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert.predict(corpus_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PREDICTION_FILE = 'prediction_BERT.csv'\n",
    "\n",
    "file_exists = os.path.exists(BERT_PREDICTION_FILE)\n",
    "\n",
    "if not file_exists:\n",
    "    # Calculate BERT predictions\n",
    "    raw_predictions_bert = model_bert.predict(corpus_test)\n",
    "\n",
    "    # Destructure probabilities for class_1 and class_2\n",
    "    predictions_class_0 = [pred[0]['score'] if pred[0]['label'] == 'LABEL_0' else pred[1]['score'] for pred in raw_predictions_bert]\n",
    "    predictions_class_1 = [pred[0]['score'] if pred[0]['label'] == 'LABEL_1' else pred[1]['score'] for pred in raw_predictions_bert]\n",
    "\n",
    "    # Create a DataFrame containing the test samples and BERT predictions\n",
    "    results = pd.DataFrame({\n",
    "        'bert_0': predictions_class_0,\n",
    "        'bert_1': predictions_class_1\n",
    "    })\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    results.to_csv(BERT_PREDICTION_FILE)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 load all prediciton in one dataframe\n",
    "so we can see which sample is most interesting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.disagreements import get_disagreements\n",
    "\n",
    "dtc_bert_sorted_by_diff,logreg_bert_sorted_by_diff,logreg_dtc_sorted_by_diff = get_disagreements(y_test,BERT_PREDICTION_FILE, LOGREG_PREDICTION_FILE, DTC_PREDICTION_FILE)\n",
    "\n",
    "# Wähle die Top-N interessanten Samples\n",
    "top_n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_bert_sorted_by_diff.head(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_bert_sorted_by_diff.head(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_dtc_sorted_by_diff.head(top_n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Analyze With SHAP\n",
    "\n",
    "We know that the Logistic Regression Performs better than the Decision Tree Classifier.\n",
    "To further investigate why thats the case we need to look into the models.\n",
    "\n",
    "In the following i will do 3 things.\n",
    "\n",
    "Per Model:\n",
    "1. Most Positive/Negative Words\n",
    "2. Investigate the Most Wrong Positive and Negative prediction\n",
    "\n",
    "Model Comparision:\n",
    "1. Investigate biggest Prediction gap accross the models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Logistic Regression with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = shap.maskers.Text(tokenizer=r\"\\W+\") # this will create a basic whitespace tokenizer\n",
    "# explainer_logreg = shap.Explainer(model_logregression_new.predict_proba , masker)\n",
    "explainer_logreg = shap.Explainer(model_logregression_new.predict_proba, masker)\n",
    "#shap_values_logreg = explainer_logreg(corpus_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Optimazation\n",
    "\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "shap_values_file = \"shap_values_logreg.pkl\"\n",
    "\n",
    "# Überprüfen, ob die Datei existiert\n",
    "if not os.path.exists(shap_values_file):\n",
    "    # Berechnen Sie die SHAP-Werte, wenn die Datei nicht existiert\n",
    "    shap_values_logreg = explainer_logreg(corpus_test)\n",
    "\n",
    "    # Speichern Sie die SHAP-Werte in einer Datei\n",
    "    with open(shap_values_file, \"wb\") as f:\n",
    "        pickle.dump(shap_values_logreg, f)\n",
    "else:\n",
    "    # Laden Sie die SHAP-Werte aus der Datei, wenn sie existiert\n",
    "    with open(shap_values_file, \"rb\") as f:\n",
    "        shap_values_logreg = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Investigate the biggest misclassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.disagreements import get_misclassifications\n",
    "logreg_misclassifications = get_misclassifications(y_test,LOGREG_PREDICTION_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_misclassifications[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Really good example to investigate why the model predicted the wrong class\n",
    "most_wrong_positive_index  = 9634"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Most Wrong Positive Classification\n",
    "\n",
    "Most wrong review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logreg_misclassifications.loc[most_wrong_positive_index])\n",
    "print(corpus_test[most_wrong_positive_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = shap_values_logreg[most_wrong_positive_index, :, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so its 0.5 behind the real label.\n",
    "But Why lets investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(explanation)\n",
    "shap.plots.waterfall(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further investigate what words really matter - highlight the words that contributed most to the prediction\n",
    "\n",
    "highlight_signal_words(explanation,round_shap_values=3,top_words=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interpretation:\n",
    "\n",
    "The model learned that the word bad and boring are negative sentiment words and they overlap all other words you can see that the review is really positive beacuse of this passage:\n",
    "`after the slow beginning and some of the soap opera antics i started liking it the plot was different than anything i had ever seen now`.\n",
    "\n",
    "So it wasnt able to understand that the word liking is really important for the sentiment.\n",
    "\n",
    "With the highlight Plot we can really see that it doesnt pay relevant attention to the relevant part."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Most Wrong Negative Classification\n",
    "\n",
    "Most wrong review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_wrong_negative_index  = 8426\n",
    "explanation = shap_values_logreg[most_wrong_negative_index, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logreg_misclassifications.loc[most_wrong_negative_index])\n",
    "print(corpus_test[most_wrong_negative_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(explanation)\n",
    "shap.plots.waterfall(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_signal_words(explanation,round_shap_values=3,top_words=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation:\n",
    "\n",
    "This was a correct classification but the initial label is wrong?\n",
    "So we found a mistake in the labeled training data.\n",
    "With this information we can remove the sample and further improve the training process\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Global Interpretation Most Positive and negative Words\n",
    "\n",
    "To understand how the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_global_explanation(shap_values_logreg[:,:,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interpretation:\n",
    "\n",
    "It seems like the model learned the correct words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analyze with SHAP: DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = shap.maskers.Text(tokenizer=r\"\\W+\") # this will create a basic whitespace tokenizer\n",
    "# explainer_logreg = shap.Explainer(model_logregression_new.predict_proba , masker)\n",
    "explainer_dtc = shap.Explainer(model_dtc.predict_proba, masker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "\n",
    "shap_values_file = \"shap_values_dtc.pkl\"\n",
    "\n",
    "# Überprüfen, ob die Datei existiert\n",
    "if not os.path.exists(shap_values_file):\n",
    "    # Berechnen Sie die SHAP-Werte, wenn die Datei nicht existiert\n",
    "    shap_values_dtc= explainer_dtc(corpus_test)\n",
    "\n",
    "    # Speichern Sie die SHAP-Werte in einer Datei\n",
    "    with open(shap_values_file, \"wb\") as f:\n",
    "        pickle.dump(shap_values_dtc, f)\n",
    "else:\n",
    "    # Laden Sie die SHAP-Werte aus der Datei, wenn sie existiert\n",
    "    with open(shap_values_file, \"rb\") as f:\n",
    "        shap_values_dtc = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Investigate Biggest Misclasssifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_misclassifications = get_misclassifications(y_test, DTC_PREDICTION_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_misclassifications[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_wrong_positive_index = 11617"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Most Wrong Positive Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = shap_values_dtc[most_wrong_positive_index, :, 1]\n",
    "print(dtc_misclassifications.loc[most_wrong_positive_index])\n",
    "print(corpus_test[most_wrong_positive_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dtc.predict_proba([corpus_test[most_wrong_positive_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explanation.base_values)\n",
    "shap.plots.text(explanation)\n",
    "shap.plots.waterfall(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_signal_words(explanation,round_shap_values=3,top_words=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpretation\n",
    "\n",
    "if you read it you know This is a positive Review. \n",
    "This Classifier Fails to understand that the review is talking about the scene and not about the movie\n",
    "you can clearly see the dominating words food they have look are all understand as negative words have great impact on the result with Shap values up to -0.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Most Wrong Negative Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_wrong_negative_index = 11488\n",
    "explanation = shap_values_dtc[most_wrong_negative_index, :, 1]\n",
    "print(dtc_misclassifications.loc[most_wrong_negative_index])\n",
    "print(corpus_test[most_wrong_negative_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(explanation)\n",
    "shap.plots.waterfall(explanation)\n",
    "highlight_signal_words(explanation,round_shap_values=3,top_words=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation \n",
    "\n",
    "This is clearly a negative review this person hates the movie.\n",
    "What confused the model was the use of great it was used in respect of the talent of the actor and what could have saved the movie: \"great momments of comedy dark humour\".\n",
    "But both uses of great wasnt about the movie."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Global Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_global_explanation(shap_values_dtc[:,:,1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interpretation:\n",
    "\n",
    "you can directly see the Shap values Words with a Positive impact in the 1 percentile are very low and many words. you can directly see that the negative words only 4 lead the sentiment so mess even worse and crap this is great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_global_explanation(shap_values_dtc[:,:,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 SHAP for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "In summary, the reason your code is not using the GPU is that the SHAP library does not support GPU acceleration for BERT models. Unfortunately, there is no direct solution to this issue. You can try looking for alternative libraries or methods that support GPU acceleration for BERT or similar text models.\n",
    " \"\"\"\n",
    "\n",
    "# Performance Optimazation\n",
    "masker = shap.maskers.Text(tokenizer=r\"\\W+\") # this will create a basic whitespace tokenizer\n",
    "# explainer_logreg = shap.Explainer(model_logregression_new.predict_proba , masker)\n",
    "explainer_bert = shap.Explainer(model_bert, masker)\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "shap_values_file = \"shap_values_bert.pkl\"\n",
    "\n",
    "# Überprüfen, ob die Datei existiert\n",
    "if not os.path.exists(shap_values_file):\n",
    "    # Berechnen Sie die SHAP-Werte, wenn die Datei nicht existiert\n",
    "    shap_values_bert= explainer_bert(corpus_test[:100])\n",
    "\n",
    "    # Speichern Sie die SHAP-Werte in einer Datei\n",
    "    with open(shap_values_file, \"wb\") as f:\n",
    "        pickle.dump(shap_values_bert, f)\n",
    "else:\n",
    "    # Laden Sie die SHAP-Werte aus der Datei, wenn sie existiert\n",
    "    with open(shap_values_file, \"rb\") as f:\n",
    "        shap_values_bert = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_bert.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Investigate Biggest Misclassifications\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_misclassifications = get_misclassifications(y_test, BERT_PREDICTION_FILE)\n",
    "dtc_misclassifications[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Biggest Positive Misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_wrong_positive_index = 19396\n",
    "\n",
    "explanation = explainer_bert([corpus_test[most_wrong_positive_index]])\n",
    "explanation = explanation[0, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtc_misclassifications.loc[most_wrong_positive_index])\n",
    "print(corpus_test[most_wrong_positive_index])\n",
    "shap.plots.text(explanation)\n",
    "shap.plots.waterfall(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_signal_words(explanation,round_shap_values=3,top_words=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretration\n",
    "\n",
    "This Classification was correct the overall sentiment of the review is negative.\n",
    "But it sounds like this was a disappointed fan who is disappointed how the last laurel and hardy movie ended.\n",
    "Nevertheless its resulted in a review with a score of > 7.\n",
    "This could be fixed in the original dataset.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Biggest Negative Misclassification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_wrong_negative_index = 3556\n",
    "\n",
    "explanation = explainer_bert([corpus_test[most_wrong_negative_index]])\n",
    "explanation = explanation[0, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtc_misclassifications.loc[most_wrong_negative_index])\n",
    "print(corpus_test[most_wrong_negative_index])\n",
    "shap.plots.text(explanation)\n",
    "shap.plots.waterfall(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_signal_words(explanation,round_shap_values=3,top_words=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "Man sieht klar das die wörter mit Positiven Sentiment einen Grossen einfuss auf das Review haben:\n",
    "enjoyed, impressed, incredible, great.\n",
    "und das das Review auch eindeutig Positiv ist\n",
    "\n",
    "Jedoch ist es trotzdem Fehlclassifikation nach weiterer Rechereche findet man das original review unter <a href=\"https://www.imdb.com/review/rw2198364/?ref_=tt_urv\">REVIEW</a>.\n",
    "Und es wurde eindeutig die Falsche anzahl an sternen vergeben"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Global Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_global_explanation(shap_values_bert[:,:,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Model Comparision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTC vs BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_bert_sorted_by_diff.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logreg vs BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_bert_sorted_by_diff.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dtc_log_reg_biggest_difference_index = 7153\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test[bert_dtc_log_reg_biggest_difference_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_explanation = explainer_dtc([corpus_test[bert_dtc_log_reg_biggest_difference_index]])\n",
    "logreg_explanation = explainer_logreg([corpus_test[bert_dtc_log_reg_biggest_difference_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DTC Explanation')\n",
    "highlight_signal_words(dtc_explanation[0,:,1],round_shap_values=3,top_words=4)\n",
    "shap.plots.text(dtc_explanation[0,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('logreg Explanation')\n",
    "highlight_signal_words(logreg_explanation[0,:,1],round_shap_values=3,top_words=4)\n",
    "shap.plots.text(logreg_explanation[0,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_explanation = explainer_bert([corpus_test[bert_dtc_log_reg_biggest_difference_index]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_explanation = bert_explanation[0, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BERT Explanation')\n",
    "highlight_signal_words(bert_explanation,round_shap_values=3,top_words=4)\n",
    "shap.plots.text(bert_explanation)\n",
    "shap.plots.waterfall( bert_explanation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## interpretation:\n",
    "\n",
    "The Worst Classification was Logistic Regression lets investigate:\n",
    "Again an example were there reviewer is talking about the movie content and this is interpreted as Negative sentiment.\n",
    "The model is not contributing anough focus on the last line overal its a good movie.\n",
    "\n",
    "The second Worst Classification was Decision Tree Classifier lets investigate:\n",
    "here its more like a 50/50 Decision the model cant really decide if its negative or positive\n",
    "the most important positive word was \"best\" and most important negative \"Directing was bad\"\n",
    "\n",
    "The best Classification was BERT:\n",
    "the biggest focus was on 7 so it understand as soon a 7 is present its like 7/10 so its a positive review.\n",
    "altough it was cheap (score: -0.26) shap understand the the reviewer thinks its overall a good movie\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HUGGING_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
